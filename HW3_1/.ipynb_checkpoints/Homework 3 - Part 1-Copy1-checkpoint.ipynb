{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Part 1\n",
    "## Decision Tree and Random forest\n",
    "\n",
    "In this homework, you will perform classification on the provided datasets using Decision Tree and Random Forest algorithms. \n",
    "\n",
    "The first dataset you will be working with contains 2 features. The second dataset contains 50 features. Both of them have a target label which can be 0 or 1.\n",
    "\n",
    "You will go step by step with the first dataset. <br>\n",
    "1 - Use a Decision Tree Classifier and observe the model performance.<br>\n",
    "2 - Use a Random Forest Classifier and observe the model performance.<br>\n",
    "3 - Use Grid Search to choose the optimal values for hyperparameters and observe the performance of the best model.\n",
    "\n",
    "\n",
    "For the second dataset, you are required to generate an optimized Random Forest model using what you have learned in the steps mentioned above.\n",
    "\n",
    "Dataset 1:\n",
    "train_2features.csv and test_2features.csv are the training set and testing set respecitvely.\n",
    "\n",
    "\n",
    "Dataset 2:\n",
    "train_50features.csv and test_50features.csv are the training set and testing set respecitvely.\n",
    "\n",
    "\n",
    "To obtain a deterministic behavior, keep the random_state in all algorithms fixed to the value given. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-10f79df1ddc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Defined functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# to ignore warnings in sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "#Basic functions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#Models and metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "#Defined functions\n",
    "from utils import * \n",
    "\n",
    "# to ignore warnings in sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# magic to not to call show every time \n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_2features.csv\")\n",
    "f, ax = visualize_2d_data(train_df)\n",
    "plt.title(\"Train Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test_2features.csv\")\n",
    "f, ax = visualize_2d_data(test_df)\n",
    "plt.title(\"Test Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. From the above visualizations, what can you tell about the need for a linear/non-linear model for classification?__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "From sklearn.tree use DecisionTreeClassifier to build a classification model with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "y_train = train_df.y.values.reshape(-1,1)\n",
    "x_train = train_df.drop(\"y\", axis=1)\n",
    "y_test = test_df.y.values.reshape(-1,1)\n",
    "x_test = test_df.drop(\"y\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the classifier on the training data\n",
    "model_tree = DecisionTreeClassifier(random_state=26)\n",
    "model_tree.fit(x_train, y_train)\n",
    "\n",
    "tree_pred_train = model_tree.predict(x_train)\n",
    "tree_pred_proba_train = model_tree.predict_proba(x_train)[::,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Print accuracy, prediction and recall for the predictions made on the training data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ###\n",
    "print(\"Accuracy on train set: \",round(accuracy_score(y_train,tree_pred_train)*100,3),\"%\")\n",
    "print(\"Precision on train set: \",round(precision_score(y_train,tree_pred_train)*100,3),\"%\")\n",
    "print(\"Recall on train set: \",round(recall_score(y_train,tree_pred_train)*100,3),\"%\")\n",
    "print(\"F1 Score on train set: \",round(f1_score(y_train,tree_pred_train)*100,3),\"%\")\n",
    "print(\"ROC AUC Score on train set: \",round(roc_auc_score(y_train,tree_pred_proba_train),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, tree_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make predictions on the testing data\n",
    "### WRITE CODE HERE ###\n",
    "tree_pred_test = model_tree.predict(x_test)\n",
    "tree_pred_proba_test = model_tree.predict_proba(x_test)[::,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Print accuracy, prediction and recall for the predictions made on the testing data.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ###\n",
    "print(\"Accuracy on train set: \",round(accuracy_score(y_test,tree_pred_test)*100,3),\"%\")\n",
    "print(\"Precision on train set: \",round(precision_score(y_test,tree_pred_test)*100,3),\"%\")\n",
    "print(\"Recall on train set: \",round(recall_score(y_test,tree_pred_test)*100,3),\"%\")\n",
    "print(\"F1 Score on train set: \",round(f1_score(y_test,tree_pred_test)*100,3),\"%\")\n",
    "print(\"ROC AUC Score on train set: \",round(roc_auc_score(y_test,tree_pred_proba_test),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, tree_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Plot ROC curve and obtain AUC for test predictions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve by giving appropriate names for title and axes. \n",
    "### WRITE CODE HERE\n",
    "fpr, tpr, _ = roc_curve(y_test,  tree_pred_proba_test)\n",
    "auc = roc_auc_score(y_test, tree_pred_proba_test)\n",
    "\n",
    "#Plotting\n",
    "plt.plot(fpr,tpr,label=\"Class '1', auc=\"+str(round(auc,3)), color='gold',lw=.8)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=.5, linestyle='--', label='Random Model')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Based on the scores for training set and test set, explain the performance of the above model in terms of bias and variance.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "\n",
    "Decision Trees have low predictive power compared to other methods due to high variance. Random Forest increases prediction power at the expense of decreased interpretability. \n",
    "\n",
    "\n",
    "From sklearn.ensemble use RandomForestClassifier to build a classification model with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the classifier on the training data\n",
    "\n",
    "model_rf = RandomForestClassifier(random_state=26)\n",
    "model_rf.fit(x_train, y_train)\n",
    "\n",
    "rf_pred_train = model_rf.predict(x_train)\n",
    "rf_pred_proba_train = model_rf.predict_proba(x_train)[::,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Print accuracy, prediction and recall for the predictions made on the training data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ###\n",
    "print(\"Accuracy on train set: \",round(accuracy_score(y_train,rf_pred_train)*100,3),\"%\")\n",
    "print(\"Precision on train set: \",round(precision_score(y_train,rf_pred_train)*100,3),\"%\")\n",
    "print(\"Recall on train set: \",round(recall_score(y_train,rf_pred_train)*100,3),\"%\")\n",
    "print(\"F1 Score on train set: \",round(f1_score(y_train,rf_pred_train)*100,3),\"%\")\n",
    "print(\"ROC AUC Score on train set: \",round(roc_auc_score(y_train,rf_pred_proba_train),3))\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_train, rf_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make predictions on the testing data\n",
    "### WRITE CODE HERE ###\n",
    "rf_pred_test = model_rf.predict(x_test)\n",
    "rf_pred_proba_test = model_rf.predict_proba(x_test)[::,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Print accuracy, prediction and recall for the predictions made on the testing data.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ###\n",
    "print(\"Accuracy on test set: \",round(accuracy_score(y_test,rf_pred_test)*100,3),\"%\")\n",
    "print(\"Precision on test set: \",round(precision_score(y_test,rf_pred_test)*100,3),\"%\")\n",
    "print(\"Recall on test set: \",round(recall_score(y_test,rf_pred_test)*100,3),\"%\")\n",
    "print(\"F1 Score on test set: \",round(f1_score(y_test,rf_pred_test)*100,3),\"%\")\n",
    "print(\"ROC AUC Score on test set: \",round(roc_auc_score(y_test,rf_pred_proba_test),3))\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, rf_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Plot ROC curve and obtain AUC for the test predictions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve by giving appropriate names for title and axes. \n",
    "### WRITE CODE HERE\n",
    "fpr, tpr, _ = roc_curve(y_test,  rf_pred_proba_test)\n",
    "auc = roc_auc_score(y_test, rf_pred_proba_test)\n",
    "\n",
    "#Plotting\n",
    "plt.plot(fpr,tpr,label=\"Class '1', auc=\"+str(round(auc,3)), color='gold',lw=.8)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=.5, linestyle='--', label='Random Model')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Based on the scores for training set and test set, explain the performance of the above model in terms of bias and variance. Is the Random Forest model better or worse than the Decision Tree model? Explain why you think the performance may have improved or deteriorated.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "\"Model tuning\" refers to model adjustments to better fit the data. This is separate from \"fitting\" or \"training\" the model. The fitting/training procedure is governed by the amount and quality of your training data, as the fitting algorithm is unique to each classifier (e.g. logistic regression or random forest). \n",
    "\n",
    "However, there are aspects of some models that are user specified. For example, when using a random forest (which is basically an ensemble of decision trees), it is probably a good idea to choose the right number of underlying trees. Too many and the model might overfit, and too few and the model might not be able to properly learn the data. Parameters such as these are referred to as \"hyperparameters\" or \"free parameters\", as the values for these are determined by the user and not the algorithm.\n",
    "\n",
    "A quick and efficient way to optimize hyperparameters is to perform Grid Search over different values of the parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below dictionary, fill in the list of values that you want to try out for each parameter\n",
    "# Refer to the descriptions in sklearn's doc to understand what the parameters depict\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [i for i in range(1,7)], #explain about 1\n",
    "    'max_features': [1, 2], #It will save time for conversion\n",
    "    'min_samples_leaf': [2,3,6],\n",
    "    'min_samples_split': [z for z in range(6,13,2)],\n",
    "    'n_estimators': [int(x) for x in np.linspace(start = 300, stop = 1100, num = 8)] #should not include more than the number of observations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf = GridSearchCV(estimator = rf, scoring='f1', param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search_rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. Display the parameters of the best model.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the best model, do the following:\n",
    "### Make predictions on the training set and display accuracy, precision and recall.\n",
    "### Make predictions on the testing set and display accuracy, precision and recall. Plot ROC curve and print AUC.\n",
    "\n",
    "### WRITE CODE HERE ###\n",
    "model_rf_best_params = RandomForestClassifier(max_depth=4 , max_features='auto' , min_samples_leaf=2 , min_samples_split=10,\n",
    "                                              n_estimators=528, random_state=26)\n",
    "model_rf_best_params.fit(x_train, y_train)\n",
    "\n",
    "best_rf_pred_train = model_rf_best_params.predict(x_train)\n",
    "best_rf_pred_proba_train = model_rf_best_params.predict_proba(x_train)[::,1]\n",
    "\n",
    "best_rf_pred_test = model_rf_best_params.predict(x_test)\n",
    "best_rf_pred_proba_test = model_rf_best_params.predict_proba(x_test)[::,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ###\n",
    "print(\"Accuracy on train set: \",round(accuracy_score(y_train,best_rf_pred_train)*100,3),\"%\")\n",
    "print(\"Precision on train set: \",round(precision_score(y_train,best_rf_pred_train)*100,3),\"%\")\n",
    "print(\"Recall on train set: \",round(recall_score(y_train,best_rf_pred_train)*100,3),\"%\")\n",
    "print(\"F1 Score on train set: \",round(f1_score(y_train,best_rf_pred_train)*100,3),\"%\")\n",
    "print(\"ROC AUC Score on train set: \",round(roc_auc_score(y_train,best_rf_pred_proba_train),3))\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_train, best_rf_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ###\n",
    "print(\"Accuracy on test set: \",round(accuracy_score(y_test,best_rf_pred_test)*100,3),\"%\")\n",
    "print(\"Precision on test set: \",round(precision_score(y_test,best_rf_pred_test)*100,3),\"%\")\n",
    "print(\"Recall on test set: \",round(recall_score(y_test,best_rf_pred_test)*100,3),\"%\")\n",
    "print(\"F1 Score on test set: \",round(f1_score(y_test,best_rf_pred_test)*100,3),\"%\")\n",
    "print(\"ROC AUC Score on test set: \",round(roc_auc_score(y_test,best_rf_pred_proba_test),3))\n",
    "print(\"-\"*80)\n",
    "print(classification_report(y_test, best_rf_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test,  best_rf_pred_proba_test)\n",
    "auc = roc_auc_score(y_test, best_rf_pred_proba_test)\n",
    "\n",
    "#Plotting\n",
    "plt.plot(fpr,tpr,label=\"Class '1', auc=\"+str(round(auc,3)), color='gold',lw=.8)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=.5, linestyle='--', label='Random Model')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q. How did performing Grid Search impact the performance of the model? Were you able to optimize the hyperparameters?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2\n",
    "\n",
    "Given this procedure, you are to optimize a random forest classifier for a dataset with 50 features. Training data are provided, but testing data does not include the labels. It is up to you to use the training data to optimize generalization performance to the test data. You will submit a csv file with your predictions. It should contain one column and the column should be named \"y\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_50features.csv\")\n",
    "print(train_df.head(2))\n",
    "\n",
    "test_data = pd.read_csv(\"test_50features.csv\")\n",
    "print(test_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_50 = train_df.drop('y',axis=1)\n",
    "y_train_50 = train_df.y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### Construct your final random forest model and optimize the hyperparameters using Grid Search ###\n",
    "model_rf_50 = RandomForestClassifier(random_state=26)\n",
    "model_rf_50.fit(x_train_50, y_train_50)\n",
    "\n",
    "rf_50_pred = model_rf_50.predict(x_train_50)\n",
    "\n",
    "print(\"Initial Accuracy on train set: \",round(accuracy_score(y_train_50,rf_50_pred)*100,3),\"%\")\n",
    "print(\"Initial Precision on train set: \",round(precision_score(y_train_50,rf_50_pred)*100,3),\"%\")\n",
    "print(\"Initial Recall on train set: \",round(recall_score(y_train_50,rf_50_pred)*100,3),\"%\")\n",
    "print(\"Initial F1 Score on train set: \",round(f1_score(y_train_50,rf_50_pred)*100,3),\"%\")\n",
    "\n",
    "print(classification_report(y_train_50, rf_50_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_50 = {\n",
    "    'max_depth': [i for i in range(3,7,1)],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [2,4,6],\n",
    "    'min_samples_split': [z for z in range(8,13,2)],\n",
    "    'n_estimators': [int(x) for x in np.linspace(start = 300, stop = 1100, num = 7)],\n",
    "    'class_weight': ['balanced', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf_50 = GridSearchCV(estimator = RandomForestClassifier(random_state=26), scoring='f1', param_grid = param_grid_50, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search_rf_50.fit(x_train_50, y_train_50)\n",
    "grid_search_rf_50.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_50_best_params = RandomForestClassifier(max_depth = 5 , max_features = 'auto', min_samples_leaf= 4, min_samples_split= 10,\n",
    "                                              n_estimators= 433,class_weight = None , random_state=26)\n",
    "\n",
    "model_rf_50_best_params.fit(x_train_50, y_train_50)\n",
    "\n",
    "rf_50_pred_bp = model_rf_50_best_params.predict(x_train_50)\n",
    "\n",
    "print(\"Initial Accuracy on train set: \",round(accuracy_score(y_train_50, rf_50_pred_bp)*100,3),\"%\")\n",
    "print(\"Initial Precision on train set: \",round(precision_score(y_train_50, rf_50_pred_bp)*100,3),\"%\")\n",
    "print(\"Initial Recall on train set: \",round(recall_score(y_train_50, rf_50_pred_bp)*100,3),\"%\")\n",
    "print(\"Initial F1 Score on train set: \",round(f1_score(y_train_50, rf_50_pred_bp)*100,3),\"%\")\n",
    "\n",
    "print(classification_report(y_train_50, rf_50_pred_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_50_pred_bp_test = model_rf_50_best_params.predict(test_data)\n",
    "np.savetxt(\"predictions.csv\", rf_50_pred_bp_test, delimiter=\",\", header='y', fmt='%g', comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
